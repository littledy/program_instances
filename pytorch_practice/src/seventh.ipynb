{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# os.getcwd()\n",
    "os.chdir('/Users/dengyi/Desktop/program_instances/pytorch_practice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "input_dim = 100\n",
    "num_channels = 1\n",
    "num_features = 64\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "itype = torch.cuda.LongTensor if use_cuda else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = range(len(test_dataset))\n",
    "indices_val = indices[:5000]\n",
    "indices_test = indices[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_val = torch.utils.data.sampler.SubsetRandomSampler(indices_val)\n",
    "sampler_test = torch.utils.data.sampler.SubsetRandomSampler(indices_test)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, sampler=sampler_val)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, sampler=sampler_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_show(img):\n",
    "    img = img.data.expand(batch_size, 3, image_size, image_size)\n",
    "    return img\n",
    "\n",
    "def imshow(inp, title=None, ax=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if inp.size()[0] > 1:\n",
    "        inp = inp.numpy.transpose((1, 2, 0))\n",
    "    else:\n",
    "        inp = inp[0].numpy()\n",
    "\n",
    "    min_value = np.amin(inp)\n",
    "    max_value = np.amax(inp)\n",
    "    if max_value > min_value:\n",
    "        inp = (inp - min_value) / (max_value - min_value)\n",
    "    ax.imshow(inp)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelG, self).__init__()\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('deconv1', nn.ConvTranspose2d(input_dim, num_features*2,\n",
    "                                                            5, 2, 0, bias=False))\n",
    "        self.model.add_module('bnorm1', nn.BatchNorm2d(num_features*2))\n",
    "        self.model.add_module('relu1', nn.ReLU(True))\n",
    "        self.model.add_module('deconv2', nn.ConvTranspose2d(num_features*2, num_features, \n",
    "                                                            5, 2, 0, bias=False))\n",
    "        self.model.add_module('bnorm2', nn.BatchNorm2d(num_features))\n",
    "        self.model.add_module('relu2', nn.ReLU(True))\n",
    "        self.model.add_module('deconv3', nn.ConvTranspose2d(num_features, num_channels, \n",
    "                                                            4, 2, 0, bias=False))\n",
    "        self.model.add_module('sigmoid', nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for name, module in self.model.named_children():\n",
    "            output = module(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if class_name.find('conv') != -1:\n",
    "        m.weight.data.normal_(0, 0.02)\n",
    "    if class_name.find('norm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ModelG()\n",
    "net = net.cuda() if use_cuda else net\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.choice(10, batch_size)\n",
    "samples = Variable(torch.from_numpy(samples).type(dtype))\n",
    "samples.resize_(batch_size, 1, 1, 1)\n",
    "samples = Variable(samples.data.expand(batch_size, input_dim, 1, 1))\n",
    "samples = samples.cuda() if use_cuda else samples\n",
    "\n",
    "def save_evaluation_samples(netModel, epoch, save_path='gan'):\n",
    "    save_path = save_path.strip()\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    fake_u = netModel(samples)\n",
    "    fake_u = fake_u.cuda() if use_cuda else fake_u\n",
    "    img = make_show(fake_u)\n",
    "    vutil.save_image(img, os.path.join(save_path, 'fake_{}.png'.format(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ModelG(target, data):\n",
    "    if use_cuda:\n",
    "        target, data = target.cuda(), data.cuda()\n",
    "    print(data.size())\n",
    "    data = data.type(dtype)\n",
    "    \n",
    "    data = data.reshape(data.size()[0]*data.size()[2]*data.size()[3], 1, 1, 1)\n",
    "    data = data.expand(data.size()[0], input_dim, 1, 1)\n",
    "    \n",
    "    net.train()\n",
    "    output = net(data)\n",
    "    loss = criterion(output, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if use_cuda:\n",
    "        loss = loss.cuda()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ModelG():\n",
    "    net.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    idx = 0\n",
    "    for (data, target) in validation_loader:\n",
    "        target, data = Variable(data), Variable(target)\n",
    "        idx += 1\n",
    "        if use_cuda:\n",
    "            target, data = target.cuda(), data.cuda()\n",
    "        data = data.type(dtype)\n",
    "        data = data.reshape(data.size()[0]*data.size()[2]*data.size()[3], 1, 1, 1)\n",
    "        data = data.expand(data.size()[0], input_dim, 1, 1)\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        if use_cuda:\n",
    "            loss = loss.cuda()\n",
    "        val_loss.append(loss.data.numpy())\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dengyi/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([50176, 1, 28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (28) must match the size of tensor b (64) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     11\u001b[0m     target, data \u001b[39m=\u001b[39m Variable(target), Variable(data)\n\u001b[0;32m---> 12\u001b[0m     loss \u001b[39m=\u001b[39m train_ModelG(target, data)\n\u001b[1;32m     13\u001b[0m     train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     14\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn [80], line 12\u001b[0m, in \u001b[0;36mtrain_ModelG\u001b[0;34m(target, data)\u001b[0m\n\u001b[1;32m     10\u001b[0m net\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m output \u001b[39m=\u001b[39m net(data)\n\u001b[0;32m---> 12\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:3291\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3289\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3291\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m   3292\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (28) must match the size of tensor b (64) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "print(\"Initialized!\")\n",
    "\n",
    "step = 0\n",
    "num_epoches = 1\n",
    "record = []\n",
    "\n",
    "for epoch in range(num_epoches):\n",
    "    train_loss = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target, data = Variable(target), Variable(data)\n",
    "        loss = train_ModelG(target, data)\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        step += 1\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            val_loss = evaluate_ModelG()\n",
    "            print('训练周期：{} [{}/{}({:.0f}%)]\\t训练数据 Loss: {:.6f}\\t校验数据 Loss：{:.6f}'.format(\n",
    "                epoch, batch_idx*batch_size, len(train_loader.dataset), 100.*batch_idx/len(train_loader), \n",
    "                np.mean(train_loss), np.mean(val_loss)\n",
    "            ))\n",
    "            record.append([np.mean(train_loss), np.mean(val_loss)])\n",
    "\n",
    "        save_evaluation_samples(net, epoch, 'MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot([i[0] for i in record], label='Training')\n",
    "plt.plot([i[1] for i in record], label='Validation')\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
